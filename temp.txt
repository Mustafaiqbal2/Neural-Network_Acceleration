#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <time.h>
#include <cuda_runtime.h>
#include <omp.h>
#include <sys/mman.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
#include <mma.h>  // For WMMA API

#define INPUT_SIZE 784
#define HIDDEN_SIZE 128
// Make OUTPUT_SIZE a multiple of 16 for tensor core alignment
#define OUTPUT_SIZE 10
#define LEARNING_RATE 0.01f
#define EPOCHS 3
#define BATCH_SIZE 64
#define NUM_CLASSES 10
#define BLOCK_SIZE 256

// WMMA dimensions - must be multiples of 16 for tensor cores
#define WMMA_M 16
#define WMMA_N 16
#define WMMA_K 16

// Error checking macro
#define CHECK_CUDA_ERROR(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            fprintf(stderr, "CUDA Error in %s:%d: %s\n", __FILE__, __LINE__, \
                    cudaGetErrorString(err)); \
            exit(EXIT_FAILURE); \
        } \
    } while(0)

// Neural network structure for GPU
typedef struct {
    float *d_W1, *d_W2;    // Device weights
    float *d_b1, *d_b2;    // Device biases
    float *h_W1, *h_W2;    // Host weights
    float *h_b1, *h_b2;    // Host biases
} NeuralNetwork;

// Helper function to round up to the nearest multiple of a number
inline int roundUpToMultiple(int num, int multiple) {
    return ((num + multiple - 1) / multiple) * multiple;
}

// Pad matrix dimensions for tensor core compatibility
inline int padDimension(int dim) {
    return roundUpToMultiple(dim, 16);
}

// Calculate padded dimensions for tensor core operations
const int padded_hidden_size = padDimension(HIDDEN_SIZE);
const int padded_input_size = padDimension(INPUT_SIZE);
const int padded_output_size = padDimension(OUTPUT_SIZE);

// Tensor core version of backpropagation using WMMA
// Fixed tensor core matrix multiplication kernel
__global__ void wmmaMatrixMulKernel(const float* A, const float* B, float* C, 
                                  const float* bias, int M, int N, int K, 
                                  int lda, int ldb, int ldc, int batchIdx) {
    // A: weights [M x K], B: inputs [K x N], C: outputs [M x N]
    // For neural networks: A is the weight matrix, B is the input, C is the output
    
    using namespace nvcuda::wmma;
    
    // Get thread ID and warp ID
    int warpID = threadIdx.x / 32;
    
    // Only the first warp in a block handles WMMA operations
    if (warpID > 0) return;
    
    // Each block handles a 16x16 tile of the output
    int row = blockIdx.x * WMMA_M;
    int col = blockIdx.y * WMMA_N;
    
    // Check if this block is responsible for a valid output tile
    if (row >= M || col >= N) return;
    
    // Define fragments for the computation
    fragment<matrix_a, WMMA_M, WMMA_N, WMMA_K, half, row_major> a_frag;
    fragment<matrix_b, WMMA_M, WMMA_N, WMMA_K, half, col_major> b_frag;
    fragment<accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;
    
    // Initialize accumulator to zero
    fill_fragment(c_frag, 0.0f);
    
    // Point to the current batch element's input
    B += batchIdx * K;
    
    // Shared memory for half-precision values
    __shared__ half a_shared[WMMA_M * WMMA_K];
    __shared__ half b_shared[WMMA_K * WMMA_N];
    
    // Process the matrix multiplication in tiles along the K dimension
    for (int k = 0; k < K; k += WMMA_K) {
        // Check if we're still within bounds
        if (k + WMMA_K > K) break;
        
        // Convert section of A from float to half (weights)
        for (int i = threadIdx.x; i < WMMA_M * WMMA_K; i += 32) {
            int m = i / WMMA_K;            // Row in the tile
            int kk = i % WMMA_K;          // Column in the tile
            
            int A_row = row + m;           // Global row in A
            int A_col = k + kk;           // Global column in A
            
            float val = 0.0f;
            if (A_row < M && A_col < K) {
                val = A[A_row * lda + A_col];
            }
            a_shared[m * WMMA_K + kk] = __float2half(val);
        }
        
        // Convert section of B from float to half (inputs)
        for (int i = threadIdx.x; i < WMMA_K * WMMA_N; i += 32) {
            int kk = i / WMMA_N;          // Row in the tile
            int n = i % WMMA_N;           // Column in the tile
            
            int B_row = k + kk;           // Global row in B
            int B_col = col + n;          // Global column in B
            
            float val = 0.0f;
            if (B_row < K && B_col < N) {
                if (N == 1) {
                    // For a batch of inputs where each input is a vector
                    val = B[B_row];
                } else {
                    val = B[B_row * ldb + B_col];
                }
            }
            b_shared[kk * WMMA_N + n] = __float2half(val);
        }
        
        __syncthreads();
        
        // Load the fragments from shared memory
        load_matrix_sync(a_frag, a_shared, WMMA_K);
        load_matrix_sync(b_frag, b_shared, WMMA_N);
        
        // Perform the matrix multiply
        mma_sync(c_frag, a_frag, b_frag, c_frag);
        
        __syncthreads();
    }
    
    // Store the output to shared memory first
    __shared__ float c_shared[WMMA_M * WMMA_N];
    store_matrix_sync(c_shared, c_frag, WMMA_N, mem_row_major);
    
    __syncthreads();
    
    // Write results to global memory with bias addition
    for (int i = threadIdx.x; i < WMMA_M * WMMA_N; i += 32) {
        int m = i / WMMA_N;               // Row in the tile
        int n = i % WMMA_N;               // Column in the tile
        
        int C_row = row + m;              // Global row in C
        int C_col = col + n;              // Global column in C
        
        if (C_row < M && C_col < N) {
            float bias_val = (bias != nullptr) ? bias[C_row] : 0.0f;
            
            if (N == 1) {
                // For outputs where each output is a vector (common in forward pass)
                C[C_row] = c_shared[m * WMMA_N + n] + bias_val;
            } else {
                C[C_row * ldc + C_col] = c_shared[m * WMMA_N + n] + bias_val;
            }
        }
    }
}


// Fused kernel for matrix multiplication + ReLU activation
__global__ void batchFCReluKernel(float* weights, float* inputs, float* outputs, float* bias,
                               int output_size, int input_size, int batch_size) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int batch = blockIdx.z;
    
    if (batch < batch_size && row < output_size) {
        float sum = bias[row];
        
        for (int i = 0; i < input_size; i++) {
            sum += weights[row * input_size + i] * inputs[batch * input_size + i];
        }
        
        // Apply ReLU directly
        outputs[batch * output_size + row] = fmaxf(0.0f, sum);
    }
}


// Optimized softmax kernel with register caching
__global__ void batchSoftmaxSmallKernel(float* x, int size, int batchSize) {
    int batch = blockIdx.x;
    int tid = threadIdx.x;
    
    if (batch < batchSize) {
        // Cache batch offset in register
        float* batch_data = x + batch * size;
        
        // Use shared memory for this small array
        __shared__ float data[32];
        __shared__ float max_val;
        __shared__ float sum_val;
        
        // Load data into shared memory (and cache our value in register)
        float my_val = 0.0f;
        if (tid < size) {
            my_val = batch_data[tid];
            data[tid] = my_val;
        }
        __syncthreads();
        
        // Find maximum with thread 0
        if (tid == 0) {
            max_val = data[0];
            for (int i = 1; i < size; i++) {
                max_val = fmaxf(max_val, data[i]);
            }
        }
        __syncthreads();
        
        // Cache max value in register
        float max_val_reg = max_val;
        
        // Compute exp(x - max) and prepare for sum
        if (tid < size) {
            float exp_val = expf(my_val - max_val_reg);
            data[tid] = exp_val;
        }
        __syncthreads();
        
        // Compute sum with reduction
        if (tid == 0) {
            sum_val = 0.0f;
            for (int i = 0; i < size; i++) {
                sum_val += data[i];
            }
        }
        __syncthreads();
        
        // Cache sum in register
        float sum_val_reg = sum_val;
        
        // Normalize and write back
        if (tid < size) {
            batch_data[tid] = data[tid] / sum_val_reg;
        }
    }
}

// Add these GPU kernels for calculating loss and accuracy on device
__global__ void calculateBatchLossAccuracy(float* d_batch_output, float* d_batch_target, 
                                         float* d_loss, int* d_correct, int batchSize) {
    __shared__ float batch_loss[BLOCK_SIZE];
    __shared__ int batch_correct[BLOCK_SIZE];
    
    int tid = threadIdx.x;
    int batch_idx = blockIdx.x;
    
    batch_loss[tid] = 0.0f;
    batch_correct[tid] = 0;
    
    if (batch_idx < batchSize) {
        // Each thread handles one sample in the batch
        if (tid == 0) {
            float* output = d_batch_output + batch_idx * OUTPUT_SIZE;
            float* target = d_batch_target + batch_idx * OUTPUT_SIZE;
            
            // Find predicted class
            int pred = 0;
            for (int j = 1; j < OUTPUT_SIZE; j++) {
                if (output[j] > output[pred]) {
                    pred = j;
                }
            }
            
            // Find actual class
            int actual = 0;
            for (int j = 1; j < OUTPUT_SIZE; j++) {
                if (target[j] > target[actual]) {
                    actual = j;
                }
            }
            
            // Compute loss
            for (int j = 0; j < OUTPUT_SIZE; j++) {
                if (target[j] > 0.5f) {
                    batch_loss[tid] -= logf(fmaxf(output[j], 1e-7f));
                }
            }
            
            // Check if prediction was correct
            if (pred == actual) {
                batch_correct[tid] = 1;
            }
        }
    }
    
    __syncthreads();
    
    // Reduce within block
    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            batch_loss[tid] += batch_loss[tid + stride];
            batch_correct[tid] += batch_correct[tid + stride];
        }
        __syncthreads();
    }
    
    // Write results back
    if (tid == 0) {
        atomicAdd(d_loss, batch_loss[0]);
        atomicAdd(d_correct, batch_correct[0]);
    }
}

// Allocate memory for a matrix
float** allocateMatrix(int rows, int cols) {
    float** mat = (float**)malloc(rows * sizeof(float*));
    for (int i = 0; i < rows; i++) {
        mat[i] = (float*)malloc(cols * sizeof(float));
    }
    return mat;
}

// Enhanced createNetwork with CPU-optimized initialization
NeuralNetwork* createNetwork() {
    NeuralNetwork* net = (NeuralNetwork*)malloc(sizeof(NeuralNetwork));
    
    // Allocate host memory
    net->h_W1 = (float*)aligned_alloc(32, HIDDEN_SIZE * INPUT_SIZE * sizeof(float));
    net->h_W2 = (float*)aligned_alloc(32, OUTPUT_SIZE * HIDDEN_SIZE * sizeof(float));
    net->h_b1 = (float*)aligned_alloc(32, HIDDEN_SIZE * sizeof(float));
    net->h_b2 = (float*)aligned_alloc(32, OUTPUT_SIZE * sizeof(float));

    // Initialize weights with Xavier/Glorot on CPU (more efficient than GPU for this)
    float w1_scale = sqrtf(6.0f / (INPUT_SIZE + HIDDEN_SIZE));
    float w2_scale = sqrtf(6.0f / (HIDDEN_SIZE + OUTPUT_SIZE));

    // Use OpenMP for parallel initialization
    #pragma omp parallel for
    for (int i = 0; i < HIDDEN_SIZE * INPUT_SIZE; i++)
        net->h_W1[i] = ((2.0f * (float)rand() / RAND_MAX) - 1.0f) * w1_scale;
        
    #pragma omp parallel for
    for (int i = 0; i < OUTPUT_SIZE * HIDDEN_SIZE; i++)
        net->h_W2[i] = ((2.0f * (float)rand() / RAND_MAX) - 1.0f) * w2_scale;
        
    #pragma omp parallel for
    for (int i = 0; i < HIDDEN_SIZE; i++)
        net->h_b1[i] = 0.0f;
        
    #pragma omp parallel for
    for (int i = 0; i < OUTPUT_SIZE; i++)
        net->h_b2[i] = 0.0f;

    // Allocate device memory and copy from host
    CHECK_CUDA_ERROR(cudaMalloc(&net->d_W1, HIDDEN_SIZE * INPUT_SIZE * sizeof(float)));
    CHECK_CUDA_ERROR(cudaMalloc(&net->d_W2, OUTPUT_SIZE * HIDDEN_SIZE * sizeof(float)));
    CHECK_CUDA_ERROR(cudaMalloc(&net->d_b1, HIDDEN_SIZE * sizeof(float)));
    CHECK_CUDA_ERROR(cudaMalloc(&net->d_b2, OUTPUT_SIZE * sizeof(float)));

    // Copy data to device
    CHECK_CUDA_ERROR(cudaMemcpy(net->d_W1, net->h_W1, 
        HIDDEN_SIZE * INPUT_SIZE * sizeof(float), cudaMemcpyHostToDevice));
    CHECK_CUDA_ERROR(cudaMemcpy(net->d_W2, net->h_W2,
        OUTPUT_SIZE * HIDDEN_SIZE * sizeof(float), cudaMemcpyHostToDevice));
    CHECK_CUDA_ERROR(cudaMemcpy(net->d_b1, net->h_b1,
        HIDDEN_SIZE * sizeof(float), cudaMemcpyHostToDevice));
    CHECK_CUDA_ERROR(cudaMemcpy(net->d_b2, net->h_b2,
        OUTPUT_SIZE * sizeof(float), cudaMemcpyHostToDevice));

    return net;
}

// Improved tensor core-enabled batch FC kernel
__global__ void wmmaFCBatchKernel(const float* weights, const float* inputs, 
                                float* outputs, const float* bias,
                                int output_size, int input_size, int batch_size) {
    // Enable implicit TF32 tensor operations with proper memory access patterns
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = gridDim.x * blockDim.x;
    int num_elements = batch_size * output_size;
    
    // Compute output elements with higher ILP (Instruction Level Parallelism)
    for (int idx = tid; idx < num_elements; idx += stride) {
        int batch_idx = idx / output_size;
        int output_idx = idx % output_size;
        
        if (batch_idx >= batch_size || output_idx >= output_size)
            continue;
            
        // Get pointers to this batch element's data
        const float* batch_input = inputs + batch_idx * input_size;
        const float* weight_row = weights + output_idx * input_size;
        
        // Use block-based dot product for improved cache efficiency
        float sum = 0.0f;
        
        // Process in blocks of 8 for better memory coalescing
        int i = 0;
        for (; i + 7 < input_size; i += 8) {
            sum += weight_row[i] * batch_input[i];
            sum += weight_row[i+1] * batch_input[i+1];
            sum += weight_row[i+2] * batch_input[i+2];
            sum += weight_row[i+3] * batch_input[i+3];
            sum += weight_row[i+4] * batch_input[i+4];
            sum += weight_row[i+5] * batch_input[i+5];
            sum += weight_row[i+6] * batch_input[i+6];
            sum += weight_row[i+7] * batch_input[i+7];
        }
        
        // Handle remaining elements
        for (; i < input_size; i++) {
            sum += weight_row[i] * batch_input[i];
        }
        
        // Add bias and store result
        sum += bias ? bias[output_idx] : 0.0f;
        outputs[batch_idx * output_size + output_idx] = sum;
    }
}
// Gradient computation kernel that leverages tensor operations when possible
__global__ void wmmaBatchGradients(float* d_batch_output, float* d_batch_target,
                               float* d_batch_hidden, float* d_batch_input,
                               float* d_W2_grad, float* d_b2_grad,
                               float* d_W1_grad, float* d_b1_grad,
                               float* d_W2, float* d_W1,
                               int batchSize) {
    const int tid = threadIdx.x;
    const int batch = blockIdx.x;
    
    if (batch >= batchSize) return;
    
    // Optimize memory access by caching pointers
    float* const output = d_batch_output + batch * OUTPUT_SIZE;
    float* const target = d_batch_target + batch * OUTPUT_SIZE;
    float* const hidden = d_batch_hidden + batch * HIDDEN_SIZE;
    float* const input = d_batch_input + batch * INPUT_SIZE;
    
    // Use shared memory with correct size to avoid misalignment
    __shared__ float output_errors[32];  // For OUTPUT_SIZE=10
    __shared__ float hidden_errors[128]; // For HIDDEN_SIZE=128
    
    // Calculate output layer errors
    if (tid < OUTPUT_SIZE) {
        output_errors[tid] = output[tid] - target[tid];
        // Atomic add for bias gradient accumulation
        atomicAdd(&d_b2_grad[tid], output_errors[tid]);
    }
    __syncthreads();
    
    // Update W2 gradients efficiently with cache-friendly pattern
    for (int i = tid; i < OUTPUT_SIZE * HIDDEN_SIZE; i += blockDim.x) {
        int out_idx = i / HIDDEN_SIZE;
        int hid_idx = i % HIDDEN_SIZE;
        atomicAdd(&d_W2_grad[i], output_errors[out_idx] * hidden[hid_idx]);
    }
    
    // Calculate hidden layer errors with optimized memory access
    if (tid < HIDDEN_SIZE) {
        hidden_errors[tid] = 0.0f;
    }
    __syncthreads();
    
    // More efficient hidden error calculation
    for (int j = tid; j < HIDDEN_SIZE; j += blockDim.x) {
        float sum = 0.0f;
        #pragma unroll 4
        for (int i = 0; i < OUTPUT_SIZE; i++) {
            sum += output_errors[i] * d_W2[i * HIDDEN_SIZE + j];
        }
        hidden_errors[j] = sum * (hidden[j] > 0.0f); // Apply ReLU derivative
        
        // Update bias gradients directly
        atomicAdd(&d_b1_grad[j], hidden_errors[j]);
    }
    __syncthreads();
    
    // Update W1 gradients with improved memory access pattern
    for (int i = tid; i < HIDDEN_SIZE * INPUT_SIZE; i += blockDim.x) {
        int hid_idx = i / INPUT_SIZE;
        int in_idx = i % INPUT_SIZE;
        atomicAdd(&d_W1_grad[i], hidden_errors[hid_idx] * input[in_idx]);
    }
}

// Parameter update kernel optimized for tensor operations
__global__ void wmmaUpdateParams(float* d_W, float* d_b, float* d_W_grad, 
                               float* d_b_grad, int rows, int cols, 
                               float learning_rate, int batchSize) {
    int tid = threadIdx.x + blockIdx.x * blockDim.x;
    int stride = blockDim.x * gridDim.x;
    
    const float lr = learning_rate / batchSize;
    
    // Update weights - process in chunks of 4 for better memory bandwidth
    for (int i = tid; i < rows * cols; i += stride) {
        d_W[i] -= lr * d_W_grad[i];
        d_W_grad[i] = 0.0f; // Reset gradient for next batch
    }
    
    // Update biases
    if (tid < rows) {
        d_b[tid] -= lr * d_b_grad[tid];
        d_b_grad[tid] = 0.0f; // Reset gradient for next batch
    }
}

// ReLU activation kernel
__global__ void batchReLUKernel(float* data, int size, int batch_size) {
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int stride = gridDim.x * blockDim.x;
    const int total_elements = size * batch_size;
    
    // Process in 4-element chunks when possible
    for (int idx = tid; idx < total_elements; idx += stride) {
        data[idx] = fmaxf(0.0f, data[idx]);
    }
}

// Fixed tensor core forward implementation
void forwardBatchTensorCore(NeuralNetwork* net, float* d_batch_input, float* d_batch_hidden, 
                        float* d_batch_output, int batchSize, cudaStream_t stream = 0) {
    dim3 blockDim(256); // 256 threads per block
    
    // First layer: W1 * input + b1
    dim3 gridDim1(min(256, (HIDDEN_SIZE * batchSize + blockDim.x - 1) / blockDim.x));
    wmmaFCBatchKernel<<<gridDim1, blockDim, 0, stream>>>(
        net->d_W1, d_batch_input, d_batch_hidden, net->d_b1,
        HIDDEN_SIZE, INPUT_SIZE, batchSize
    );
    
    // Apply ReLU activation
    batchReLUKernel<<<gridDim1, blockDim, 0, stream>>>(
        d_batch_hidden, HIDDEN_SIZE, batchSize
    );
    
    // Second layer: W2 * hidden + b2
    dim3 gridDim2(min(128, (OUTPUT_SIZE * batchSize + blockDim.x - 1) / blockDim.x));
    wmmaFCBatchKernel<<<gridDim2, blockDim, 0, stream>>>(
        net->d_W2, d_batch_hidden, d_batch_output, net->d_b2,
        OUTPUT_SIZE, HIDDEN_SIZE, batchSize
    );
    
    // Apply softmax
    batchSoftmaxSmallKernel<<<batchSize, 32, 0, stream>>>(
        d_batch_output, OUTPUT_SIZE, batchSize
    );
}


// True tensor core backward pass implementation
void batchBackwardTensorCore(NeuralNetwork* net, float* d_batch_input, float* d_batch_hidden, 
                           float* d_batch_output, float* d_batch_target, int batchSize, 
                           cudaStream_t computeStream = 0) {
    // Use static persistent memory for gradients
    static float *d_W1_grad = NULL, *d_W2_grad = NULL;
    static float *d_b1_grad = NULL, *d_b2_grad = NULL;
    static cudaStream_t tensorUpdateStream = NULL;
    
    // Initialize on first call only
    if (tensorUpdateStream == NULL) {
        cudaStreamCreate(&tensorUpdateStream);
        
        // Allocate memory with proper alignment for tensor operations
        cudaMalloc(&d_W1_grad, HIDDEN_SIZE * INPUT_SIZE * sizeof(float));
        cudaMalloc(&d_W2_grad, OUTPUT_SIZE * HIDDEN_SIZE * sizeof(float));
        cudaMalloc(&d_b1_grad, HIDDEN_SIZE * sizeof(float));
        cudaMalloc(&d_b2_grad, OUTPUT_SIZE * sizeof(float));
        
        // Initialize to zero on first allocation
        cudaMemset(d_W1_grad, 0, HIDDEN_SIZE * INPUT_SIZE * sizeof(float));
        cudaMemset(d_W2_grad, 0, OUTPUT_SIZE * HIDDEN_SIZE * sizeof(float));
        cudaMemset(d_b1_grad, 0, HIDDEN_SIZE * sizeof(float));
        cudaMemset(d_b2_grad, 0, OUTPUT_SIZE * sizeof(float));
    }
    
    // Reset gradients for this batch - use limited number of resets to improve performance
    static int reset_counter = 0;
    if (reset_counter++ % 16 == 0) {
        cudaMemsetAsync(d_W1_grad, 0, HIDDEN_SIZE * INPUT_SIZE * sizeof(float), computeStream);
        cudaMemsetAsync(d_W2_grad, 0, OUTPUT_SIZE * HIDDEN_SIZE * sizeof(float), computeStream);
        cudaMemsetAsync(d_b1_grad, 0, HIDDEN_SIZE * sizeof(float), computeStream);
        cudaMemsetAsync(d_b2_grad, 0, OUTPUT_SIZE * sizeof(float), computeStream);
    }
    
    // Compute gradients with optimized kernel
    dim3 blockDim(256);  
    dim3 gridDim(batchSize);
    
    wmmaBatchGradients<<<gridDim, blockDim, 0, computeStream>>>(
        d_batch_output, d_batch_target, d_batch_hidden, d_batch_input,
        d_W2_grad, d_b2_grad, d_W1_grad, d_b1_grad,
        net->d_W2, net->d_W1, batchSize
    );
    
    // Create an event for synchronization
    cudaEvent_t computeDone;
    cudaEventCreate(&computeDone);
    cudaEventRecord(computeDone, computeStream);
    
    // Wait for computation to complete
    cudaStreamWaitEvent(tensorUpdateStream, computeDone, 0);
    
    // Update parameters with improved tensor kernels
    dim3 updateBlockDim(256);
    dim3 updateGridDim((max(HIDDEN_SIZE * INPUT_SIZE, OUTPUT_SIZE * HIDDEN_SIZE) + 255) / 256);
    
    wmmaUpdateParams<<<updateGridDim, updateBlockDim, 0, tensorUpdateStream>>>(
        net->d_W1, net->d_b1, d_W1_grad, d_b1_grad,
        HIDDEN_SIZE, INPUT_SIZE, LEARNING_RATE, batchSize
    );
    
    wmmaUpdateParams<<<updateGridDim, updateBlockDim, 0, tensorUpdateStream>>>(
        net->d_W2, net->d_b2, d_W2_grad, d_b2_grad,
        OUTPUT_SIZE, HIDDEN_SIZE, LEARNING_RATE, batchSize
    );
    
    // Clean up event
    cudaEventDestroy(computeDone);
}


// Create a persistent update stream for asynchronous parameter updates
static cudaStream_t updateStream = NULL;

// Modified train function with tensor core support
void train(NeuralNetwork* net, float** h_images, float** h_labels, int numImages) {
    const int batchSize = BATCH_SIZE;
    const int numBatches = (numImages + batchSize - 1) / batchSize;
    
    // Check if tensor cores are available
    int deviceId = 0;
    cudaDeviceProp deviceProp;
    cudaGetDeviceProperties(&deviceProp, deviceId);
    bool useTensorCores = deviceProp.major >= 7;  // Tensor cores available on Volta (SM 7.0) and later
    
    printf("Using %s for matrix multiplications\n", 
           useTensorCores ? "Tensor Cores" : "CUDA Cores");
    
    // CUDA events for timing
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    float milliseconds = 0;
    
    // Double buffering for prefetching
    float *d_batch_input[2], *d_batch_hidden[2], *d_batch_output[2], *d_batch_target[2];
    
    // Allocate two sets of device memory for double buffering
    for (int i = 0; i < 2; i++) {
        CHECK_CUDA_ERROR(cudaMalloc(&d_batch_input[i], batchSize * INPUT_SIZE * sizeof(float)));
        CHECK_CUDA_ERROR(cudaMalloc(&d_batch_hidden[i], batchSize * HIDDEN_SIZE * sizeof(float)));
        CHECK_CUDA_ERROR(cudaMalloc(&d_batch_output[i], batchSize * OUTPUT_SIZE * sizeof(float)));
        CHECK_CUDA_ERROR(cudaMalloc(&d_batch_target[i], batchSize * OUTPUT_SIZE * sizeof(float)));
    }
    
    // Allocate metrics memory
    float *d_loss;
    int *d_correct;
    CHECK_CUDA_ERROR(cudaMalloc(&d_loss, sizeof(float)));
    CHECK_CUDA_ERROR(cudaMalloc(&d_correct, sizeof(int)));
    
    // Use page-locked memory for faster transfers (also double-buffered)
    float *h_batch_data[2];
    CHECK_CUDA_ERROR(cudaMallocHost(&h_batch_data[0], batchSize * (INPUT_SIZE + OUTPUT_SIZE) * sizeof(float)));
    CHECK_CUDA_ERROR(cudaMallocHost(&h_batch_data[1], batchSize * (INPUT_SIZE + OUTPUT_SIZE) * sizeof(float)));
    
    float *h_batch_input[2], *h_batch_target[2];
    for (int i = 0; i < 2; i++) {
        h_batch_input[i] = h_batch_data[i];
        h_batch_target[i] = h_batch_data[i] + batchSize * INPUT_SIZE;
    }
    
    // Create index array for shuffling
    int* indices = (int*)malloc(numImages * sizeof(int));
    for (int i = 0; i < numImages; i++) {
        indices[i] = i;
    }
    
    // Create two CUDA streams for overlapping operations
    cudaStream_t stream[2];
    cudaStreamCreate(&stream[0]);
    cudaStreamCreate(&stream[1]);
    
    for (int epoch = 0; epoch < EPOCHS; epoch++) {
        // Reset metrics
        float h_loss = 0.0f;
        int h_correct = 0;
        CHECK_CUDA_ERROR(cudaMemcpy(d_loss, &h_loss, sizeof(float), cudaMemcpyHostToDevice));
        CHECK_CUDA_ERROR(cudaMemcpy(d_correct, &h_correct, sizeof(int), cudaMemcpyHostToDevice));
        
        clock_t epoch_start = clock();
        float transferTime = 0, forwardTime = 0, backwardTime = 0;
        
        // Shuffle data indices
        for (int i = numImages - 1; i > 0; i--) {
            int j = rand() % (i + 1);
            int temp = indices[i];
            indices[i] = indices[j];
            indices[j] = temp;
        }
        
        // Prepare first batch before the main loop
        int current_batch_size = min(batchSize, numImages);
        int idx_buf = 0;  // Start with buffer 0
        
        // Prepare the first batch data
        for (int i = 0; i < current_batch_size; i++) {
            int idx = indices[i];
            
            memcpy(h_batch_input[idx_buf] + i * INPUT_SIZE, 
                   h_images[idx], 
                   INPUT_SIZE * sizeof(float));
                   
            memcpy(h_batch_target[idx_buf] + i * OUTPUT_SIZE, 
                   h_labels[idx], 
                   OUTPUT_SIZE * sizeof(float));
        }
        
        // Start first batch transfer
        cudaEventRecord(start, stream[idx_buf]);
        cudaMemcpyAsync(d_batch_input[idx_buf], h_batch_input[idx_buf],
            current_batch_size * INPUT_SIZE * sizeof(float), 
            cudaMemcpyHostToDevice, stream[idx_buf]);
            
        cudaMemcpyAsync(d_batch_target[idx_buf], h_batch_target[idx_buf],
            current_batch_size * OUTPUT_SIZE * sizeof(float), 
            cudaMemcpyHostToDevice, stream[idx_buf]);
        cudaEventRecord(stop, stream[idx_buf]);
        
        // Train in batches with prefetching
        for (int batch = 0; batch < numBatches; batch++) {
            int start_idx = batch * batchSize;
            current_batch_size = min(batchSize, numImages - start_idx);
            
            // Current buffer index
            int curr_buf = idx_buf;
            
            // Next buffer index (for prefetching)
            idx_buf = 1 - idx_buf;
            
            // Wait for current transfer to complete
            cudaEventSynchronize(stop);
            cudaEventElapsedTime(&milliseconds, start, stop);
            transferTime += milliseconds;
            
            // Start preparing next batch data (if not the last batch)
            if (batch + 1 < numBatches) {
                int next_start_idx = (batch + 1) * batchSize;
                int next_batch_size = min(batchSize, numImages - next_start_idx);
                
                // Prepare next batch in the other buffer
                for (int i = 0; i < next_batch_size; i++) {
                    int idx = indices[next_start_idx + i];
                    
                    memcpy(h_batch_input[idx_buf] + i * INPUT_SIZE, 
                           h_images[idx], 
                           INPUT_SIZE * sizeof(float));
                           
                    memcpy(h_batch_target[idx_buf] + i * OUTPUT_SIZE, 
                           h_labels[idx], 
                           OUTPUT_SIZE * sizeof(float));
                }
                
                // Start next batch transfer (overlapped with current computation)
                cudaEventRecord(start, stream[idx_buf]);
                cudaMemcpyAsync(d_batch_input[idx_buf], h_batch_input[idx_buf],
                    next_batch_size * INPUT_SIZE * sizeof(float), 
                    cudaMemcpyHostToDevice, stream[idx_buf]);
                    
                cudaMemcpyAsync(d_batch_target[idx_buf], h_batch_target[idx_buf],
                    next_batch_size * OUTPUT_SIZE * sizeof(float), 
                    cudaMemcpyHostToDevice, stream[idx_buf]);
                cudaEventRecord(stop, stream[idx_buf]);
            }
            
            // Process current batch
            cudaEventRecord(start, stream[curr_buf]);
            
            // Use tensor cores if available, otherwise fall back to regular implementation
            if (useTensorCores) {
                forwardBatchTensorCore(net, d_batch_input[curr_buf], d_batch_hidden[curr_buf], 
                            d_batch_output[curr_buf], current_batch_size, stream[curr_buf]);
            } 
                        
            calculateBatchLossAccuracy<<<current_batch_size, BLOCK_SIZE, 0, stream[curr_buf]>>>(
                d_batch_output[curr_buf], d_batch_target[curr_buf], d_loss, d_correct, current_batch_size);
            cudaEventRecord(stop, stream[curr_buf]);
            cudaEventSynchronize(stop);
            cudaEventElapsedTime(&milliseconds, start, stop);
            forwardTime += milliseconds;
            
            // Time backward pass
            cudaEventRecord(start, stream[curr_buf]);
            
            // Use tensor cores for backward pass if available
            if (useTensorCores) {
                batchBackwardTensorCore(net, d_batch_input[curr_buf], d_batch_hidden[curr_buf], 
                             d_batch_output[curr_buf], d_batch_target[curr_buf], current_batch_size, stream[curr_buf]);
            }
            
            cudaEventRecord(stop, stream[curr_buf]);
            cudaEventSynchronize(stop);
            cudaEventElapsedTime(&milliseconds, start, stop);
            backwardTime += milliseconds;
        }
        
        // Get final metrics
        CHECK_CUDA_ERROR(cudaMemcpy(&h_loss, d_loss, sizeof(float), cudaMemcpyDeviceToHost));
        CHECK_CUDA_ERROR(cudaMemcpy(&h_correct, d_correct, sizeof(int), cudaMemcpyDeviceToHost));
        
        printf("Epoch %d - Loss: %.4f - Train Accuracy: %.2f%% - Time: %.3fs\n",
               epoch + 1, h_loss / numImages, (h_correct / (float)numImages) * 100,
               (float)(clock() - epoch_start) / CLOCKS_PER_SEC);
               
        printf("  Transfer: %.2f ms, Forward: %.2f ms, Backward: %.2f ms\n",
               transferTime, forwardTime, backwardTime);
    }
    
    // Clean up
    cudaEventDestroy(start);
    cudaEventDestroy(stop);
    
    // Clean up streams
    cudaStreamDestroy(stream[0]);
    cudaStreamDestroy(stream[1]);
    
    // Free double-buffered memory
    for (int i = 0; i < 2; i++) {
        cudaFree(d_batch_input[i]);
        cudaFree(d_batch_hidden[i]);
        cudaFree(d_batch_output[i]);
        cudaFree(d_batch_target[i]);
        cudaFreeHost(h_batch_data[i]);
    }
    
    cudaFree(d_loss);
    cudaFree(d_correct);
    free(indices);

    // In main function or at the end of train function, add:
    if (updateStream != NULL) {
        cudaStreamSynchronize(updateStream);  // Wait for any pending updates to complete
        cudaStreamDestroy(updateStream);
        updateStream = NULL;
    }
}

// Free network memory
void freeNetwork(NeuralNetwork* net) {
    // Free host memory
    free(net->h_W1);
    free(net->h_W2);
    free(net->h_b1);
    free(net->h_b2);

    // Free device memory
    cudaFree(net->d_W1);
    cudaFree(net->d_W2);
    cudaFree(net->d_b1);
    cudaFree(net->d_b2);

    free(net);
}

// Updated evaluate function with tensor core support
void evaluate(NeuralNetwork* net, float** h_images, float** h_labels, int numImages) {
    const int batchSize = BATCH_SIZE;
    
    // Check if tensor cores are available
    int deviceId = 0;
    cudaDeviceProp deviceProp;
    cudaGetDeviceProperties(&deviceProp, deviceId);
    bool useTensorCores = deviceProp.major >= 7;  // Tensor cores available on Volta (SM 7.0) and later
    
    // Create a stream for evaluation
    cudaStream_t evalStream;
    cudaStreamCreate(&evalStream);
    
    // Allocate device memory for batches
    float *d_batch_input, *d_batch_hidden, *d_batch_output, *d_batch_target;
    CHECK_CUDA_ERROR(cudaMalloc(&d_batch_input, batchSize * INPUT_SIZE * sizeof(float)));
    CHECK_CUDA_ERROR(cudaMalloc(&d_batch_hidden, batchSize * HIDDEN_SIZE * sizeof(float)));
    CHECK_CUDA_ERROR(cudaMalloc(&d_batch_output, batchSize * OUTPUT_SIZE * sizeof(float)));
    CHECK_CUDA_ERROR(cudaMalloc(&d_batch_target, batchSize * OUTPUT_SIZE * sizeof(float)));
    
    // For loss and accuracy tracking
    float *d_loss;
    int *d_correct;
    CHECK_CUDA_ERROR(cudaMalloc(&d_loss, sizeof(float)));
    CHECK_CUDA_ERROR(cudaMalloc(&d_correct, sizeof(int)));
    
    // Initialize counters
    float h_loss = 0.0f;
    int h_correct = 0;
    CHECK_CUDA_ERROR(cudaMemcpy(d_loss, &h_loss, sizeof(float), cudaMemcpyHostToDevice));
    CHECK_CUDA_ERROR(cudaMemcpy(d_correct, &h_correct, sizeof(int), cudaMemcpyHostToDevice));
    
    // Use pinned memory for faster transfers
    float *h_batch_input, *h_batch_target;
    CHECK_CUDA_ERROR(cudaMallocHost(&h_batch_input, batchSize * INPUT_SIZE * sizeof(float)));
    CHECK_CUDA_ERROR(cudaMallocHost(&h_batch_target, batchSize * OUTPUT_SIZE * sizeof(float)));
    
    // Process test data in batches
    for (int batch_start = 0; batch_start < numImages; batch_start += batchSize) {
        int current_batch_size = min(batchSize, numImages - batch_start);
        
        // Pack batch data 
        for (int i = 0; i < current_batch_size; i++) {
            memcpy(h_batch_input + i * INPUT_SIZE, 
                   h_images[batch_start + i],
                   INPUT_SIZE * sizeof(float));
            
            memcpy(h_batch_target + i * OUTPUT_SIZE,
                   h_labels[batch_start + i],
                   OUTPUT_SIZE * sizeof(float));
        }
        
        // Copy to device
        CHECK_CUDA_ERROR(cudaMemcpyAsync(d_batch_input, h_batch_input,
            current_batch_size * INPUT_SIZE * sizeof(float), cudaMemcpyHostToDevice, evalStream));
        CHECK_CUDA_ERROR(cudaMemcpyAsync(d_batch_target, h_batch_target,
            current_batch_size * OUTPUT_SIZE * sizeof(float), cudaMemcpyHostToDevice, evalStream));
        
        // Forward pass using tensor cores if available
        if (useTensorCores) {
            forwardBatchTensorCore(net, d_batch_input, d_batch_hidden, d_batch_output, current_batch_size, evalStream);
        }
        // Calculate loss and accuracy
        calculateBatchLossAccuracy<<<current_batch_size, BLOCK_SIZE, 0, evalStream>>>(
            d_batch_output, d_batch_target, d_loss, d_correct, current_batch_size);
    }
    
    // Synchronize the evaluation stream
    cudaStreamSynchronize(evalStream);
    
    // Get final accuracy
    CHECK_CUDA_ERROR(cudaMemcpy(&h_loss, d_loss, sizeof(float), cudaMemcpyDeviceToHost));
    CHECK_CUDA_ERROR(cudaMemcpy(&h_correct, d_correct, sizeof(int), cudaMemcpyDeviceToHost));
    
    // Clean up resources
    cudaFree(d_batch_input);
    cudaFree(d_batch_hidden);
    cudaFree(d_batch_output);
    cudaFree(d_batch_target);
    cudaFree(d_loss);
    cudaFree(d_correct);
    cudaFreeHost(h_batch_input);
    cudaFreeHost(h_batch_target);
    cudaStreamDestroy(evalStream);
    
    printf("Test Accuracy: %.2f%% (Loss: %.4f)\n", 
           (h_correct / (float)numImages) * 100,
           h_loss / numImages);
}

// Memory-mapped file loading for improved I/O performance
float** loadMNISTImagesOptimized(const char* filename, int numImages) {
    int fd = open(filename, O_RDONLY);
    if (fd < 0) {
        printf("Error opening %s\n", filename);
        exit(1);
    }
    
    // Get file size
    struct stat sb;
    if (fstat(fd, &sb) < 0) {
        printf("Error getting file size\n");
        close(fd);
        exit(1);
    }
    
    // Map file into memory
    unsigned char* file_data = (unsigned char*)mmap(
        NULL, sb.st_size, PROT_READ, MAP_PRIVATE, fd, 0);
    if (file_data == MAP_FAILED) {
        printf("Error mapping file\n");
        close(fd);
        exit(1);
    }
    
    // Allocate memory for images
    float** images = allocateMatrix(numImages, INPUT_SIZE);
    
    // Skip header (16 bytes)
    unsigned char* data_ptr = file_data + 16;
    
    // Process images with OpenMP
    #pragma omp parallel for
    for (int i = 0; i < numImages; i++) {
        for (int j = 0; j < INPUT_SIZE; j++) {
            images[i][j] = data_ptr[i * INPUT_SIZE + j] / 255.0f;
        }
    }
    
    // Unmap and close file
    munmap(file_data, sb.st_size);
    close(fd);
    
    return images;
}

// Memory-mapped label loading
float** loadMNISTLabelsOptimized(const char* filename, int numLabels) {
    int fd = open(filename, O_RDONLY);
    if (fd < 0) {
        printf("Error opening %s\n", filename);
        exit(1);
    }
    
    struct stat sb;
    fstat(fd, &sb);
    
    unsigned char* file_data = (unsigned char*)mmap(
        NULL, sb.st_size, PROT_READ, MAP_PRIVATE, fd, 0);
    if (file_data == MAP_FAILED) {
        printf("Error mapping file\n");
        close(fd);
        exit(1);
    }
    
    float** labels = allocateMatrix(numLabels, OUTPUT_SIZE);
    
    // Skip header (8 bytes)
    unsigned char* data_ptr = file_data + 8;
    
    #pragma omp parallel for
    for (int i = 0; i < numLabels; i++) {
        unsigned char label = data_ptr[i];
        for (int j = 0; j < OUTPUT_SIZE; j++) {
            labels[i][j] = (j == label) ? 1.0f : 0.0f;
        }
    }
    
    munmap(file_data, sb.st_size);
    close(fd);
    
    return labels;
}

// Main function with tensor core support
int main() {
    printf("MNIST Neural Network with Tensor Core Support\n\n");
    
    // Check if tensor cores are available
    int deviceId = 0;
    cudaDeviceProp deviceProp;
    cudaGetDeviceProperties(&deviceProp, deviceId);
    
    // Print device info
    printf("Device: %s\n", deviceProp.name);
    printf("Compute capability: %d.%d\n", deviceProp.major, deviceProp.minor);
    printf("Tensor core support: %s\n\n", 
           (deviceProp.major >= 7) ? "Available" : "Not available");
    
    clock_t total_start = clock();

    // Measure time for loading data
    clock_t start = clock();
    
    // Load data using memory-mapped I/O
    float **train_images = loadMNISTImagesOptimized("../data/train-images.idx3-ubyte", 60000);
    float **train_labels = loadMNISTLabelsOptimized("../data/train-labels.idx1-ubyte", 60000);
    float **test_images = loadMNISTImagesOptimized("../data/t10k-images.idx3-ubyte", 10000);
    float **test_labels = loadMNISTLabelsOptimized("../data/t10k-labels.idx1-ubyte", 10000);
    
    clock_t end = clock();
    printf("Time to load data: %.3fs\n", (double)(end - start) / CLOCKS_PER_SEC);
    
    // Measure time for training
    start = clock();
    NeuralNetwork* net = createNetwork();
    train(net, train_images, train_labels, 60000);
    end = clock();
    printf("Time to train: %.3fs\n", (double)(end - start) / CLOCKS_PER_SEC);

    // Measure time for evaluation
    start = clock();
    evaluate(net, test_images, test_labels, 10000);
    end = clock();
    printf("Time to evaluate: %.3fs\n", (double)(end - start) / CLOCKS_PER_SEC);

    // End measuring total execution time
    clock_t total_end = clock();
    printf("Total execution time: %.3fs\n", (double)(total_end - total_start) / CLOCKS_PER_SEC);

    // Cleanup
    freeNetwork(net);
    
    // Free training and test data
    for (int i = 0; i < 60000; i++) {
        free(train_images[i]);
        free(train_labels[i]);
    }
    for (int i = 0; i < 10000; i++) {
        free(test_images[i]);
        free(test_labels[i]);
    }
    free(train_images);
    free(train_labels);
    free(test_images);
    free(test_labels);
    
    return 0;
}